{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83dcb7d7",
   "metadata": {},
   "source": [
    "### 1. What type of node is consider pure in decision tree?\n",
    "#### In decision trees, a node is considered pure if all the data in that node belongs to a single class. A node in the tree is considered pure if, in 100 percent of the cases, the nodes fall into a specific category of the target field. The accuracy of the decision tree can change based on the depth of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e244ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9508b48",
   "metadata": {},
   "source": [
    "### 2. What are the advantages of using Decision Tree?\n",
    "#### Some of the advantages of decision trees are:\n",
    "1. They are easy to read and interpret without requiring statistical knowledge.\n",
    "2. They force the consideration of all possible outcomes of a decision and trace each path to a conclusion.\n",
    "3. They take less effort for data preparation compared to other decision techniques.\n",
    "4. They require less data cleaning as they can handle missing values and outliers.\n",
    "5. They can be used for both classification and regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13aecb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "847e9c77",
   "metadata": {},
   "source": [
    "### 3. What is the difference between gini impurity and information gain? Explain Entropy also.\n",
    "#### Gini impurity and entropy are both measures of how mixed or pure a set of items with different classes is. They are used by different decision tree algorithms to select the best features for splitting the data. Gini impurity ranges from 0 to 0.5, while entropy ranges from 0 to 1. Gini impurity requires less computational power than entropy and penalizes less small impurities.\n",
    "1. Gini index operates on the categorical target variables in terms of `success` or `failure` and performs only binary split, in opposite to that Information Gain computes the difference between entropy before and after the split and indicates the impurity in classes of elements.\n",
    "2. Both gini and entropy are measures of impurity of a node. A node having multiple classes is impure whereas a node having only one class is pure.\n",
    "3. Entropy is a measure of disorder or uncertainty in a system. It is used in decision trees to calculate the homogeneity of a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f819c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb716aa8",
   "metadata": {},
   "source": [
    "### 4. What do you need to prune the decision tree?\n",
    "#### Pruning is a technique that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant. Pruning helps to reduce the risk of overfitting and improve predictive accuracy. Pruning requires a separate data set from the one used for growing the tree. Pruning can be done by selecting the nodes that you want to prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a15aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a2428c",
   "metadata": {},
   "source": [
    "### 5. What is the difference between post-pruning and pre-pruning?\n",
    "#### Pre-pruning or early stopping involves halting the tree before it is fully grown, while post-pruning refers to pruning the tree after it has grown to its full potential. Pre-pruning stops the non-significant branches from generating, while post-pruning generates the full tree and then removes the non-significant branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8315e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27a638c6",
   "metadata": {},
   "source": [
    "### 6. Explain how ID3 produces classification trees?\n",
    "#### The ID3 algorithm is used by training on a data set to produce a decision tree which is stored in memory. At runtime, this decision tree is used to classify new test cases by traversing the decision tree using the features of the datum to arrive at a leaf node.\n",
    "1. To select best features ID3 used Information Gain. Information Gain calculates the reduction in the entropy and measures how well a given feature separates or classifies the target classes. The feature with the highest Information Gain is selected as the best one.\n",
    "2. Entropy is the measure of disorder and the Entropy of a dataset is the measure of disorder in the target feature of the dataset.\n",
    "3. In the case of binary classification (where the target column has only two types of classes) entropy is 0 if all values in the target column are homogenous(similar) and will be 1 if the target column has equal number values for both the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313ffc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7633d1e2",
   "metadata": {},
   "source": [
    "### 7. What is the difference between ID3 and C4.5 algorithms?\n",
    "#### ID3 and C4.5 are decision tree building algorithms for Classification Models from data. \n",
    "- The ID3 algorithm determines the classification of objects by testing the values of their properties, using categorical attributes.\n",
    "- C4.5 is an improvement of ID3, making it able to handle real-valued attributes and missing attributes.\n",
    "- ID3 only works with Discrete or nominal data, but C4.5 works with both Discrete and Continuous data. C4.5 also deals with missing values and pruning trees after construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba77500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a136cc1",
   "metadata": {},
   "source": [
    "### 8. How would you deal with an overfitted Decision tree?\n",
    "#### To deal with overfitting in decision trees:\n",
    "1. Simplify input data by experimenting with different features and finding the minimum set of features that keeps the model performant enough and does not cause overfitting.\n",
    "2. Optimize your model hyperparameters by trying different regularization parameters.\n",
    "3. Use pre-pruning or post-pruning to avoid overfitting. Pre-pruning generates a tree with fewer branches than would otherwise be the case, while post-pruning generates a tree in full and then removes parts of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fc9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ab5a3c4",
   "metadata": {},
   "source": [
    "### 9. Explain how the CART algorithm performs the pruning?\n",
    "#### The CART algorithm performs pruning by cutting back the tree. After a tree has been built, it may be overfitted, so the CART algorithm will repeatedly partition data into smaller and smaller subsets until those final subsets are homogeneous in terms of the outcome variable. The CART algorithm recognizes candidate subtrees through a procedure of repeated pruning, and prunes first those branches supporting the least more predictive power per leaf. It recognizes these least beneficial branches based on a concept known as the adjusted error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72a0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d71a067",
   "metadata": {},
   "source": [
    "### 10. Explain the measure of goodness used by CART.\n",
    "#### The measure of \"goodness\" is a function that seeks to optimize the balance of a candidate split's capacity to create pure children with its capacity to create equally-sized children. This process is repeated for each impure node until the tree is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d87a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e8cd531",
   "metadata": {},
   "source": [
    "### 11. Explain the difference between the CART and ID3 algorithm.\n",
    "#### Difference between these algorithm:\n",
    "1. Type of learning:\n",
    "- ID3(Iterative Dichotomiser) is for binary classification only. CART(Classification And Regression Trees) is a family of algorithms (including, but not limited to, binary classification tree learning).\n",
    "2. Loss functions used for split selection:\n",
    "- ID3 as other comments have mentioned, selects its splits based on Information Gain, which is the reduction in entropy between the parent node and (weighted sum of) children nodes. CART, when used for classification, selects its splits to achieve the subsets that minimize Gini impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9869aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7407223a",
   "metadata": {},
   "source": [
    "### 12. What are the limitations of CART?\n",
    "#### Limitations of CART algorithm are,\n",
    "1. Main limitations is that it is prone to overfitting on the training data if its growth is not restricted in some way. This problem is typically handled by pruning the tree, which in effect regularizes the model. Care needs to be taken to ensure the pruned tree performs as we want on unseen data.\n",
    "2. Tree structure may be unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17301b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1434f97d",
   "metadata": {},
   "source": [
    "### 13. What are the advantages of CART?\n",
    "#### Advantages of CART include123:\n",
    "1. Results are simplistic.\n",
    "2. Classification and regression trees are Nonparametric and Nonlinear.\n",
    "3. Classification and regression trees implicitly perform feature selection.\n",
    "4. Outliers have no meaningful effect on CART.\n",
    "5. It requires minimal supervision and produces easy-to-understand models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9783d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
