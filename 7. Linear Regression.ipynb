{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d51a1c8",
   "metadata": {},
   "source": [
    "### 1. What are the assumptions of Linear Regression?\n",
    "#### The assumptions of linear regression are:\n",
    "- Linear relationship between the dependent(y) and independent(x) variables.\n",
    "- Multivariate normality - all variables in a linear regression analysis are normally distributed.\n",
    "- No or little multicollinearity among the independent variables.\n",
    "- No auto-correlation in the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4a839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a3f7ed0",
   "metadata": {},
   "source": [
    "### 2. What is the difference between R square and adjusted R square?\n",
    "#### R-squared and Adjusted R-squared are both measurements of the variation in a regression model. The main difference between the two is that R-squared increases every time you add an independent variable to a model, even if the variable is insignificant, whereas Adjusted R-squared increases only when the independent variable is significant and affects the dependent variable. Adjusted R-squared is a new version of R-squared that adjusts the variable predictors in regression models. Adjusted R-squared is preferred by many investment professionals because it has the potential to be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f23eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d58c82",
   "metadata": {},
   "source": [
    "### 3. What are the disadvantages of linear model?\n",
    "#### Disadvantages to using linear models are:\n",
    "- They can be simple model and fail to capture the complexity of the data.\n",
    "- They are sensitive to outliers and multicollinearity, which can affect the accuracy and stability of the coefficients.\n",
    "- They can underfit or overfit the data, which can lead to poor generalization and prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c0923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ad53065",
   "metadata": {},
   "source": [
    "### 4. What is the difference between Ridge & Lasso Regression?\n",
    "#### Ridge and Lasso regression are techniques for regularizing linear regression models and preventing overfitting. They both add a penalty term to the cost function, but with different approaches. Ridge regression shrinks the coefficients towards zero, while Lasso regression encourages some of them to be exactly zero. Ridge regression takes the square of the coefficients, while Lasso regression takes the magnitude of the coefficients. Ridge regression only reduces the coefficients close to zero but not zero, whereas Lasso regression can reduce coefficients of some features to zero, thus resulting in better feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e16df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea1c568f",
   "metadata": {},
   "source": [
    "### 5. What will be the impact of multicollinearity on linear regression model?\n",
    "#### Multicollinearity is a problem that affects linear regression models when one or more of the regressors are highly correlated with linear combinations of other regressors. When this happens, the OLS estimator of the regression coefficients tends to be very imprecise, that is, it has high variance, even if the sample size is large. In linear regression analysis, no two variables or predictors can share an exact relationship in any manner. Thus, when multicollinearity occurs, it negatively affects the regression analysis model, and the researchers obtain unreliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00843f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4389425",
   "metadata": {},
   "source": [
    "### 6. How to find the multicollinearity?\n",
    "#### To check multicollinearity, you can:\n",
    "1. Calculate correlation coefficients for all pairs of predictor variables and look for values close to or exactly +1 or -1.\n",
    "2. Review scatterplot and correlation matrices to see the types of relationships between the predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc81d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1abfea5c",
   "metadata": {},
   "source": [
    "### 7. What is MSE and RMSE? Why we calculate both of these?\n",
    "#### RMSE and MSE are two metrics to quantify how well a model fits a dataset. RMSE is the square root of MSE, which means it has the same units as the target variable. RMSE is more easily interpreted and communicated to end users than MSE. MSE is a combination of bias and variance of the prediction. MSE is a differentiable function that makes it easy to perform mathematical operations. RMSE is more sensitive to outliers than MSE. The lower the RMSE or MSE, the better a model fits a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d291a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff3bbfe",
   "metadata": {},
   "source": [
    "### 8. If You have one independent variable. How many coefficient will you required to estimate in the simple linear regression model?\n",
    "#### 2 coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813c2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6728c3a4",
   "metadata": {},
   "source": [
    "### 9. Why do we use optimization? Explain.\n",
    "#### Optimization is used in linear regression to minimize a cost function with respect to the model parameters (i.e. the weights and bias). A linear regression model can be trained using the optimization algorithm gradient descent by iteratively modifying the modelâ€™s parameters to reduce the mean squared error (MSE) of the model on a training dataset. In the case of linear regression, the coefficients can be found by least squares optimization, which can be solved using linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d5614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
